[
  {
    "objectID": "posts/source-sink/index.html",
    "href": "posts/source-sink/index.html",
    "title": "Source-sink behavioural dynamics limit institutional evolution in a group structured society",
    "section": "",
    "text": "resdb = FileAttachment(\"source-sink-res.db\").sqlite()\n\n\n\n\n\n\n\nf = (x) => Number.isInteger(x) ? x.toPrecision(2) : x\nminmax = (p, i) => d3.extent(p.map(d => parseFloat(d[i])))\ns = (p,i) => { \n  const unique_vals = Array.from(new Set(p.map(d => parseFloat(d[i]))))\n                           .sort((a,b) => a - b)\n  const out = []\n  for (let i=1; i < unique_vals.length; i++ ) {\n    out.push(+(unique_vals[i]-unique_vals[i-1]).toPrecision(1))\n  } // return whatev if length is zero\n  return out.length === 0 ? 0.1 : out[0]\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 1Model 2Model 3\n\n\n\nDescription\nThe key ingredients of the model are our groups \\(G\\) with the number of adopters \\(i\\) and with an institution of level \\(\\ell\\). We assume that with higher levels of institutional strength, \\(\\ell\\), the institution will more effectively promote group-beneficial behavior, \\(\\ell\\)\\(\\beta\\). As it gets better, each adopter in the group also gain a collective benefit \\(b\\). But all of these toodily-doo perks are offset by an institutional implementation costs, \\(c\\), of entertaining larger groups. For instance, think of the process of unionization, promoting behaviors that are costly at individual level. When unionization becomes more successful, the unions can become ungaingly. Lastly adopters lose their behavioural trait at a rate \\(\\gamma\\).\nFirst master equation1:\n\\[\\begin{align*}\n\\frac{d}{dt}G_{i,\\ell}^{diff} &= \\ell \\mathbin{\\color{darkgreen}{\\beta}} [(i-1) + R](n - i + 1)G_{i-1,\\ell} \\\\\n                              &- \\ell\\mathbin{\\color{darkgreen}{\\beta}} (i+R)(n-i) G_{i,\\ell} \\\\\n                              &+ \\mathbin{\\color{red}{\\gamma}}(i+1)G_{i+1,\\ell} - \\mathbin{\\color{red}{\\gamma}} i G_{i,\\ell}\n\\end{align*}\\]\nwhere \\(R = \\mathbin{\\color{blue}{\\rho}} \\sum_{i',\\ell'} i'G_{i',\\ell'}\\) represents the global diffusion of behaviors and primes denote variable over which we sum to calculate global quantity. The sum over adopters at each level weighted by global behavioural diffusion \\(\\rho\\).\nSecond master equation:\n\\[\\begin{align*}\n\\frac{d}{dt}G_{i,\\ell}^{select} &= \\mathbin{\\color{blue}{\\rho}} [G_{i,\\ell-1}(Z_\\ell Z_{\\ell-1}^{-1} + \\mathbin{\\color{midnightblue}{\\mu}}) + G_{i,\\ell+1}(Z\\ell Z_{\\ell + 1}^{-1} + \\mathbin{\\color{midnightblue}{\\mu}})] \\\\\n                                &-\\mathbin{\\color{blue}{\\rho}}(Z_{\\ell-1}Z_\\ell^{-1} + Z_{\\ell+1}^{-1} + 2\\mathbin{\\color{midnightblue}{\\mu}})G_{i,\\ell}\n\\end{align*}\\]\nwhere \\(Z_\\ell = \\frac{\\sum_{i'} exp(\\mathbin{\\color{seagreen}{b}}i'- \\mathbin{\\color{darkred}{c}}\\ell)G_{i',\\ell}}{\\sum_{i'}G_{i',\\ell}}\\). Note that we add a constant rate of transition \\(\\mu\\) to the selection proces.\nTaken together we have the set of master equations:\n\\[\n\\frac{d}{dt}G_{i,\\ell} = \\frac{d}{dt}G_{i,\\ell}^{diff} + \\frac{d}{dt}G_{i,\\ell}^{select}\n\\]\n\n\nJulia model\nfunction source_sink!(du, u, p, t)\n    G, L, n = u, length(u.x), length(first(u.x))\n    β, γ, ρ, b, c, μ = p\n    Z, pop, R = zeros(L), zeros(L), 0.\n\n    # Calculate mean-field coupling and observed fitness landscape\n    for ℓ in 1:L\n      n_adopt = collect(0:(n-1))\n      Z[ℓ]    = sum(exp.(b*n_adopt .- c*(ℓ-1)) .* G.x[ℓ])\n      pop[ℓ]  = sum(G.x[ℓ])\n      R       += sum(ρ*n_adopt .* G.x[ℓ])\n      pop[ℓ] > 0.0 && ( Z[ℓ] /= pop[ℓ] )\n    end\n\n    for ℓ = 1:L, i = 1:n\n      n_adopt, gr_size = i-1, n-1\n\n      # Diffusion events\n      du.x[ℓ][i] = -γ*n_adopt*G.x[ℓ][i] - (ℓ-1)*β*(n_adopt+R)*(gr_size-n_adopt)*G.x[ℓ][i]\n\n      n_adopt > 0 && ( du.x[ℓ][i] += β*(ℓ-1)*(n_adopt-1+R)*(gr_size-n_adopt+1)*G.x[ℓ][i-1])\n      n_adopt < gr_size && ( du.x[ℓ][i] +=  γ*(n_adopt+1)*G.x[ℓ][i+1] )\n\n      # Group selection process\n      ℓ > 1 && ( du.x[ℓ][i] += ρ*G.x[ℓ-1][i]*(Z[ℓ] / Z[ℓ-1] + μ) - ρ*G.x[ℓ][i]*(Z[ℓ-1] / Z[ℓ]+μ) )\n      ℓ < L && ( du.x[ℓ][i] += ρ*G.x[ℓ+1][i]*(Z[ℓ] / Z[ℓ+1] + μ) - ρ*G.x[ℓ][i]*(Z[ℓ+1] / Z[ℓ]+μ) )\n    end\nend\n\n\nAnalysis\n\n\nunique_name = resdb.query(`SELECT DISTINCT name FROM sourcesink1`)\np = unique_name.map(d => d.name.split(\"_\"))\n\nviewof beta  = Inputs.range(minmax(p,0), {step: s(p,0),  label: \"β\", value:p[0][0]})\nviewof gamma = Inputs.range(minmax(p,1), {step: s(p,1),  label: \"γ\", value:p[0][1]})\nviewof rho   = Inputs.range(minmax(p,2), {step: s(p,2),  label: \"ρ\", value:p[0][2]})\nviewof b     = Inputs.range(minmax(p,3), {step: s(p,3),  label: \"b\", value:p[0][3]})\nviewof c     = Inputs.range(minmax(p,4), {step: s(p,4),  label: \"c\", value:p[0][4]})\nviewof mu    = Inputs.range(minmax(p,5), {step: s(p,5),  label: \"μ\", value:p[0][5]})\n\ndata = resdb.query(`\n  SELECT * \n  FROM sourcesink1\n  WHERE\n  name = '${f(beta)}_${f(gamma)}_${f(rho)}_${f(b)}_${f(c)}_${f(mu)}'\n`)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  x: {type:\"log\"},\n  y: {domain: [0,1]},\n  color: {scheme: \"reds\", type: \"ordinal\", legend: true},\n  marks: [\n    Plot.line(\n      data, {\n        x: 'timestep', y: \"value\", stroke: \"L\"\n        }),\n    Plot.dot(\n      data, {\n        x: 'timestep', y: \"value\", stroke: \"L\"\n        })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\nTakeaways:\n\nFrequency of behaviour in groups with different institutional strength.\nWithin groups, the frequency of cooperative behaviour follows the strength of institutions (with ℓ = 1 in light beige and ℓ = 6 in dark red).\nQualitatively, no institutions are possible if institutional costs are too high, and the behaviour never spreads.\nThe time dynamics of global behavioural frequency and behaviour in groups can include patterns of surge and collapse.\n\n\n\n\n\nDescription\nThe key difference in that model from the last is that contagion is something to be limited by institutions of various levels. As such, \\(\\beta\\) in our model now must be negative while \\(\\alpha\\) must be positive for transmission to fall with \\(\\ell\\).\nWe ask ourselves to what extent the contagion is able to spread with very little \\(\\beta\\) values.\nWe want institutions to be able to stop contagions but contagion must exist in the first place.\nfunction source_sink2!(du, u, p, t)\n    G, L, n = u, length(u.x), length(first(u.x))\n    β, α, γ, ρ, b, c, μ = p\n    Z, pop, R = zeros(L), zeros(L), 0.\n\n    # Calculate mean-field coupling and observed fitness landscape\n    for ℓ in 1:L\n        n_adopt = collect(0:(n-1))\n        Z[ℓ]    = sum(exp.(b*n_adopt .- c*(ℓ-1)) .* G.x[ℓ]) \n        pop[ℓ]  = sum(G.x[ℓ])\n        R      += sum(ρ * n_adopt .* G.x[ℓ]) \n        pop[ℓ] > 0.0 && ( Z[ℓ] /= pop[ℓ] ) \n      end\n      \n      for ℓ = 1:L, i = 1:n\n        n_adopt, gr_size = i-1, n-1\n        # Diffusion events\n        du.x[ℓ][i] = -γ*n_adopt*G.x[ℓ][i] - β*(ℓ^-α)*(n_adopt+R)*(gr_size-n_adopt)*G.x[ℓ][i]\n        n_adopt > 0 && ( du.x[ℓ][i] += β*(ℓ^-α)*(n_adopt-1+R)*(gr_size-n_adopt+1)*G.x[ℓ][i-1])\n        n_adopt < gr_size && ( du.x[ℓ][i] +=  γ*(n_adopt+1)*G.x[ℓ][i+1] )\n        # Group selection process\n        ℓ > 1 && ( du.x[ℓ][i] += ρ*G.x[ℓ-1][i]*(Z[ℓ] / Z[ℓ-1] + μ) - ρ*G.x[ℓ][i]*(Z[ℓ-1] / Z[ℓ]+μ) )\n        ℓ < L && ( du.x[ℓ][i] += ρ*G.x[ℓ+1][i]*(Z[ℓ] / Z[ℓ+1] + μ) - ρ*G.x[ℓ][i]*(Z[ℓ+1] / Z[ℓ]+μ) )\n      end\nend\n\n\nPlot\n\n\nunique_name2 = resdb.query(`SELECT DISTINCT name FROM sourcesink2`)\np2 = unique_name2.map(d => d.name.split(\"_\"))\n\nviewof beta2  = Inputs.range(minmax(p2,0), {step: s(p2,0),  label: \"β\", value:p2[0][0]})\nviewof alpha  = Inputs.range(minmax(p2,1), {step: s(p2,1),  label: \"α\", value:p2[0][1]})\nviewof gamma2 = Inputs.range(minmax(p2,2), {step: s(p2,2),  label: \"γ\", value:p2[0][2]})\nviewof rho2   = Inputs.range(minmax(p2,3), {step: s(p2,3),  label: \"ρ\", value:p2[0][3]})\nviewof b2     = Inputs.range(minmax(p2,4), {step: s(p2,4),  label: \"b\", value:p2[0][4]})\nviewof c2     = Inputs.range(minmax(p2,5), {step: s(p2,5),  label: \"c\", value:p2[0][5]})\nviewof mu2    = Inputs.range(minmax(p2,6), {step: s(p2,6),  label: \"μ\", value:p2[0][6]})\n\ndata2 = resdb.query(`\n  SELECT * \n  FROM sourcesink2\n  WHERE\n  name = '${f(beta2)}_${f(alpha)}_${f(gamma2)}_${f(rho2)}_${f(b2)}_${f(c2)}_${f(mu2)}'\n`)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  x: {type:\"log\"},\n  marginLeft: 50,\n  color: {scheme: \"reds\", type: \"ordinal\", legend: true},\n  marks: [\n    Plot.line(\n      data2, {\n        x: 'timestep', y: \"value\", stroke: \"L\"\n        }),\n    Plot.dot(\n      data2, {\n        x: 'timestep', y: \"value\", stroke: \"L\"\n        })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescription\nThe key difference in that model from the last is that contagion is something to be limited by institutions of various levels. As such, \\(\\beta\\) in our model now must be negative while \\(\\alpha\\) must be positive for transmission to fall with \\(\\ell\\).\nWe ask ourselves to what extent the contagion is able to spread with very little \\(\\beta\\) values.\nWe want institutions to be able to stop contagions but contagion must exist in the first place.\nfunction source_sink3!(du, u, p, t)\n  G, L, n = u, length(u.x), length(u.x[1])\n  β, γ, ρ, b, c, μ = p\n  Z, pop, R = zeros(L), zeros(L), 0.\n\n  # Calculate mean-field coupling and observed fitness landscape\n  # In the following, the functions g (cost-benefits for groups) and g̃ (fitness function) are taken equal to function f. The three have similar properties.\n    for ℓ in 1:L\n      n_adopt = collect(0:(n-1))\n      Z[ℓ]    = sum(f.(b*n_adopt .- c*(ℓ-1)) .* G.x[ℓ])\n      pop[ℓ]  = sum(G.x[ℓ])\n      R      += sum(n_adopt .* G.x[ℓ]) # Global diffusion\n      pop[ℓ] > 0.0 && ( Z[ℓ] /= pop[ℓ] )\n  end\n\n    for ℓ = 1:L, i = 1:n\n      n_adopt, gr_size = i-1, n-1\n      # Inndividual selection process\n      du.x[ℓ][i] = -n_adopt*f(1-h(ℓ))*G.x[ℓ][i] - (gr_size-n_adopt)*f(h(ℓ)-1)*G.x[ℓ][i]\n      du.x[ℓ][i] += -n_adopt*(gr_size-n_adopt)*(β+γ)*G.x[ℓ][i] - ρ*(gr_size-n_adopt)*β*R*G.x[ℓ][i] - ρ*n_adopt*γ*(gr_size-R)*G.x[ℓ][i]\n      n_adopt > 0 && ( du.x[ℓ][i] += (gr_size-n_adopt+1)*f(h(ℓ)-1)*G.x[ℓ][i-1] + β*(n_adopt-1+ρ*R)*(gr_size-n_adopt+1)*G.x[ℓ][i-1] )\n      n_adopt < gr_size && ( du.x[ℓ][i] += (n_adopt+1)*f(1-h(ℓ))*G.x[ℓ][i+1] + γ*(gr_size-n_adopt-1+ρ*(gr_size-R))*(n_adopt+1)*G.x[ℓ][i+1] )\n      # Group selection process\n      ℓ > 1 && ( du.x[ℓ][i] += f(b*n_adopt-c*(ℓ-1))*(μ+ρ*Z[ℓ]/Z[ℓ-1])*G.x[ℓ-1][i] - (μ*f(c*(ℓ-1)-b*n_adopt)+ρ*f(b*n_adopt-c*(ℓ-2))*Z[ℓ-1]/Z[ℓ])*G.x[ℓ][i] )\n      ℓ < L && ( du.x[ℓ][i] += (μ*f(c*ℓ-b*n_adopt)+ρ*f(b*n_adopt-c*(ℓ-1))*Z[ℓ]/Z[ℓ+1])*G.x[ℓ+1][i] - f(b*n_adopt-c*ℓ)*(μ+ρ*Z[ℓ+1]/Z[ℓ])*G.x[ℓ][i] )\n    end\nend\n\n\nPlot\n\n\nunique_name3 = resdb.query(`SELECT DISTINCT name FROM sourcesink3`)\np3 = unique_name3.map(d => d.name.split(\"_\"))\n\nviewof beta3  = Inputs.range(minmax(p3,0), {step: s(p3,0),  label: \"β\", value:p3[0][0]})\nviewof gamma3 = Inputs.range(minmax(p3,1), {step: s(p3,1),  label: \"γ\", value:p3[0][1]})\nviewof rho3   = Inputs.range(minmax(p3,2), {step: s(p3,2),  label: \"ρ\", value:p3[0][2]})\nviewof b3     = Inputs.range(minmax(p3,3), {step: s(p3,3),  label: \"b\", value:p3[0][3]})\nviewof c3     = Inputs.range(minmax(p3,4), {step: s(p3,4),  label: \"c\", value:p3[0][4]})\nviewof mu3    = Inputs.range(minmax(p3,5), {step: s(p3,5),  label: \"μ\", value:p3[0][5]})\n\ndata3 = resdb.query(`\n  SELECT * \n  FROM sourcesink3\n  WHERE\n  name = '${f(beta3)}_${f(beta3)}_${f(rho3)}_${f(b3)}_${f(c3)}_${f(mu3)}'\n`)\n\nInputs.table(data3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  x: {type:\"log\"},\n  marginLeft: 50,\n  color: {scheme: \"reds\", type: \"ordinal\", legend: true},\n  marks: [\n    Plot.line(\n      data3, {\n        x: 'timestep', y: \"value\", stroke: \"L\"\n        }),\n    Plot.dot(\n      data3, {\n        x: 'timestep', y: \"value\", stroke: \"L\"\n        })\n  ]\n})\n\n\n\n\n\n\n\n\n\nNote that \\(\\gamma = \\beta\\) in all cases.\n\n\nTakeaways:\n\n\n\n\n\n\n\n\n\n\nModel 1 Sketch\n\n\n\n\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\nA sidenote on master equations for non-physicists. The citation for master equations in the original paper is the following:\n\nHébert-Dufresne, L., Noël, P.-A., Marceau, V., Allard, A., & Dubé, L. J. (2010). Propagation dynamics on networks featuring complex topologies. Physical Review E, 82(3), 036115. https://doi.org/10.1103/PhysRevE.82.036115\n\nThe term ‘’master equation’’ is not mentionned once in the paper. But they do talk about “a mean-field description used to coherently couple the dynamics of the network elements (nodes, vertices, individuals…) and their recurrent topological patterns (subgraphs, groups…)” that yields a set of ODEs for the time evolution of the system. Another paper writen by Guillaume St-Onge et al. is a more generous in their description of master equation:\n\nSt-Onge, G., Thibeault, V., Allard, A., Dubé, L. J., & Hébert-Dufresne, L. (2021). Master equation analysis of mesoscopic localization in contagion dynamics on higher-order networks. Physical Review E, 103(3), 032301. https://doi.org/10.1103/PhysRevE.103.032301\n\nIn it, section II does a great job of describing what master equations are and why they are powerful modeling tools. Relevant to this model, we learn that the size of a group is determined by drawing from a group size distribution. This is what we do in our intialization scheme above. We also learn that these 3 following papers are relevant to understand master equations:\n\nLindquist, J., Ma, J., van den Driessche, P., & Willeboordse, F. H. (2011). Effective degree network disease models. Journal of Mathematical Biology, 62(2), 143–164. https://doi.org/10.1007/s00285-010-0331-2  Gleeson, J. P. (2011). High-Accuracy Approximation of Binary-State Dynamics on Networks. Physical Review Letters, 107(6), 068701. https://doi.org/10.1103/PhysRevLett.107.068701  Marceau, V., Noël, P.-A., Hébert-Dufresne, L., Allard, A., & Dubé, L. J. (2010). Adaptive networks: Coevolution of disease and topology. Physical Review E, 82(3), 036116. https://doi.org/10.1103/PhysRevE.82.036116\n\n↩︎"
  },
  {
    "objectID": "posts/sci-group-life-cycle/index.html",
    "href": "posts/sci-group-life-cycle/index.html",
    "title": "Scientific group life cycle",
    "section": "",
    "text": "Model Sketch\n\n\n\n\n\n\n\n\n\nThere are research groups \\(G\\) with a number of non-programmers \\(n\\) and programmers \\(p\\). In a data-driven world, we assume that learning to code confer a large benefit to programmers over non-programmer such that \\(\\alpha << \\beta\\). There is a constant rate of influx of students who do not know how to learn to code in research groups \\(\\mu\\). There is a cost of learning to code \\(c(p,n)\\), which depend on the number of programmers and non-programmers within group. We assume that programmers and non-programmers have different graduation rates, \\(\\nu_p\\) and \\(\\nu_n\\), with \\(\\nu_p > \\nu_n\\).\nWe model the group life cycle with the following master equation:\n\\[\\begin{align*}\n\\frac{d}{dt}G_{n,p} &= \\mu(G_{n-1,p} - G_{n,p}) + \\nu_n \\Big((n+1)G_{n+1,p}-nG_{n,p}\\Big) \\\\\n                           &+ \\Big[ \\tau_g(n+1,p-1)(1-c(n+1, p-1)G_{n+1,p-1} - \\tau_g(n,p)G_{n,p} \\Big] \\\\\n                   &+ \\nu_p\\Big((p+1)G_{n,p+1} - pG_{n,p} \\Big) \\\\\n                   &+ \\tau_g(n+1,p)(1-c(n+1,p))G_{n+1,p}\n\\end{align*}\\]\nLearning to code confers a collective benefits on individuals \\(\\tau_g(n,p; \\alpha, \\beta) \\propto \\frac{\\bar{Z}_{n,p}}{Z_{n,p}}\\), where\n\\[\\log(Z_{n,p}) \\sim \\alpha * n + \\beta * p\\] \\[\\log(\\bar{Z}_{n,p}) \\sim \\alpha (n-1) +\\beta (c * p + (1-c)(p+1))\\]\nWe can think of \\(\\bar{Z}_{n,p}\\) as the potential benefits over \\(Z_{n,p}\\). Reorganizing the terms, we get:\n\\[\\begin{align*}\n\\log[\\tau_g(n,p; \\alpha, \\beta))] &= \\alpha (n-1) +\\beta (c * p + (1-c)(p+1)) - \\alpha * n + \\beta * p \\\\\n                                  &= -\\alpha + \\beta(1-c)\n\\end{align*}\\]\nNote that \\(\\tau_g\\) ends up being a function of \\(n, p\\) through the cost function: \\[c(n,p) = c_0*e^{-\\frac{p}{n}}\\]\nYou can explore both functions below:\n\n\n\n\n\n\nCost function\n\n\n\n\n\n\n\nfunction cost_prog(n, i, c_0) { return c_0 * Math.exp(-i/n); }\nfunction cost_prog2(n, i, c_0) { return c_0 * Math.exp(-i/(n+i)); }\n\nmax_gr_size = 20\nviewof N = Inputs.range([1, max_gr_size], {value: 20, step: 1, label: \"group Size\"})\nviewof coder = Inputs.range([0, (N-1)], {value: 10, step: 1, label: \"# coder\"})\nviewof c_0 = Inputs.range([0, 1], {value: 0.95, step: 0.01, label: \"c₀\"})\nviewof nc = Inputs.range([1, N], {value: (N-coder), step: 1., label: \"# non-coder\", disabled:true})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nc(n,p) = c₀ * exp(-p/n)c(n,p) = c₀ * exp(-p/(n+p))\n\n\n\nnon_coder = N - coder\nxs = [...Array(N).keys()];\nys = xs.map(x => cost_prog(non_coder, x, c_0))\n\n\nPlot.lineY(ys).plot({\n    height: 400, width: 450, grid: true,\n    y: { label: \"↑ c(n,p)\", domain:[0,1] },\n    x: { label: \"p/n →\" }\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(p/n\\) \\(\\Rightarrow\\) /  =   \\(c(n,p)\\) =  (c=1 means that non-coders always fail to learn to code; c=0 means non-coders always succeed) Non-programmers can still learn to code when \\(p=0\\) because of \\(c_0\\) I woudl expect a bigger difference when we go from no prorammers in the team to one programmer\n\n\n\nx2s = [...Array(N).keys()];\ny2s = x2s.map(x => cost_prog2(non_coder, x, c_0))\n\nPlot.lineY(y2s).plot({\n    height: 400, width: 450, grid: true,\n    y: { label: \"↑ c(n,p)\", domain:[0,1] },\n    x: { label: \"p/n →\" }\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(p/n\\) \\(\\Rightarrow\\) /  =   \\(c(n,p)\\) =  (c=1 means that non-coders always fail to learn to code; c=0 means non-coders always succeed) Non-programmers can still learn to code when \\(p=0\\) because of \\(c_0\\) I woudl expect a bigger difference when we go from no prorammers in the team to one programmer\n\n\n\n\n\n\n\n\n\n\n\n\n\nGroup benefits\n\n\n\n\n\n\n\nfunction tau(n, i, alpha, beta) {\n    const c = cost_prog(n, i, 1)\n    return Math.exp(-alpha + beta*(1 - c))\n}\n\ntau_max_gr_size = 20\nviewof tau_alpha = Inputs.range([2, 4], {value: 1., step: 1, label: \"α\", format: x => 10**-x})\nviewof tau_beta = Inputs.range([1, 3], {value: 1., step: 1, label: \"β\", format: x => 10**-x})\nviewof tau_N = Inputs.range([0, tau_max_gr_size], {value: 20, step: 1, label: \"group Size\"})\nviewof tau_coder = Inputs.range([1, tau_max_gr_size], {value: 10, step: 1, label: \"# coder\"})\nviewof tau_nc = Inputs.range([1, max_gr_size], {value: (tau_N-tau_coder), step: 1., label: \"# non-coder\", disabled:true})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nτ(n,p) 1\n\n\n\ntau_non_coder = tau_N - tau_coder\ntau_xs = [...Array(tau_N).keys()];\ntau_ys = tau_xs.map(x => tau(tau_non_coder, x, 10**-tau_alpha, 10**-tau_beta))\n\nPlot.lineY(tau_ys).plot({\n    height: 400, width: 450, grid: true,\n    y: { label: \"↑ τ(α,β;n,p)\" },\n    x: { label: \"p/n →\" }\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(p/n\\) \\(\\Rightarrow\\) /  ="
  },
  {
    "objectID": "posts/sci-group-life-cycle/index.html#julia-model",
    "href": "posts/sci-group-life-cycle/index.html#julia-model",
    "title": "Scientific group life cycle",
    "section": "Julia model",
    "text": "Julia model\n\n\n\n\n\n\nInitialization scheme\n\n\n\n\n\nfunction initialize_u0(;N::Int=20)\n  N += 1 # add column for zeroth case\n  G = zeros(N, N)\n  \n  for i=1:N, j=1:N\n    G[i,j] = 1/(N*N)\n  end\n  return ArrayPartition(Tuple([G[n,:] for n=1:N]))\nend\n\nμ  = 0.001   # inflow new students-non coders\nνₙ = 0.01    # death rate non-coders\nνₚ = 0.05    # death rate coders\nα  = 0.01    # benefits non coders\nβ  = 0.1     # benefits coders\np  = [μ, νₙ, νₚ, α, β]\n\nn = 9\nu₀ = initialize_u0(N=n)\ntspan = (0., 1000.)\n\n\n\nc(n, i) = 0.95 * exp(-i / n) # cost function\nτ(n, i, α, β) = exp(-α + β*(1 - c(n, i))) # group benefits\n\nfunction life_cycle_research_groups!(du, u, p, t)\n\n  G, N, P = u, length(u.x), length(first(u₀.x)) # Note that there can be no coders but not non-coders\n  μ, νₙ, νₚ, α, β = p\n  for n=1:N, i=1:P\n    println(\"n:$(n), i:$(i), G.x[n][i]:$(G.x[n][i])\")\n    coder, non_coder = i-1, n-1   # we distinguish indices from actual values.\n    \n    du.x[n][i] = 0\n\n    non_coder > 0 && ( du.x[n][i] += μ*(G.x[n-1][i]) )                # 1st term\n    \n    # for everybody\n    # println(\"2: $(νₙ*non_coder*G.x[n][i])\")\n    du.x[n][i] -= νₙ*non_coder*G.x[n][i]\n    # println(\"3: $(νₚ*coder*G.x[n][i])\")\n    du.x[n][i] -= νₚ*coder*G.x[n][i]\n\n    # upper boxes don't exist \n    if i < P\n      # non_coder > 0 && println(\"4: $(τ(non_coder, coder, α, β)*G.x[n][i] )\")\n      # We don't want to pass non_coder = 0 to τ()\n      non_coder > 0 && ( du.x[n][i] -= τ(non_coder, coder, α, β)*G.x[n][i] )               # 4th term\n      # println(\"5: $(νₚ*(coder+1)*G.x[n][i+1])\")\n      du.x[n][i] += νₚ*(coder+1)*G.x[n][i+1]  # 5th term\n    end\n    \n    # the bottom boxes don't exist\n    if n < N\n      # println(\"6: $(μ*G.x[n][i])\")\n      du.x[n][i] -= μ*G.x[n][i]                                       # 1st term\n      du.x[n][i] += τ(non_coder+1, coder, α, β)*(c(non_coder+1, coder))*G.x[n+1][i]     # 6th term\n      du.x[n][i] += νₙ*(non_coder+1)*G.x[n+1][i]                                            # 2nd term\n      coder > 0 && ( du.x[n][i] += τ(non_coder+1, coder-1, α, β)*(1-c(non_coder+1, coder-1))*G.x[n+1][i-1] ) # 3rd term \n    end\n  end\nend"
  },
  {
    "objectID": "posts/sci-group-life-cycle/index.html#output",
    "href": "posts/sci-group-life-cycle/index.html#output",
    "title": "Scientific group life cycle",
    "section": "Output",
    "text": "Output\n\ndata = FileAttachment(\"data.json\").json()\np = Object.keys(data).map(d => d.split(\"_\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nminmax = (p, i) => d3.extent(p.map(d => parseFloat(d[i])))\n\nviewof N_    = Inputs.range(minmax(p,0), {step: 1, label: \"N\", value:\"4\"})\nviewof mu    = Inputs.range(minmax(p,1), {step: 0.03, label: \"μ\", value:\"0.0001\"})\nviewof nu_n  = Inputs.range(minmax(p,2), {step: 0.05, label: \"νₙ\", value:\"0.01\"})\nviewof nu_p  = Inputs.range(minmax(p,3), {step: 0.1,  label: \"νₚ\", value:\"0.05\"})\nviewof alpha = Inputs.range(minmax(p,4), {step: 0.15, label: \"α\", value:\"0.01\"})\nviewof beta  = Inputs.range(minmax(p,5), {step: 0.05, label: \"β\", value:\"0.1\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot\n\n\n\nf = (x) => x.toPrecision() ? x.toPrecision(2) : x\n\nPlot.plot({\n  x: {type:\"log\"},\n  y: {domain: [0,1]},\n  color: {scheme: \"reds\", type: \"ordinal\", legend: true},\n  marks: [\n    Plot.line(\n      data[`${N_}_${mu}_${nu_n}_${nu_p}_${alpha}_${beta}`], {\n        x: 'timesteps', y: \"value\", stroke: \"N\"\n        }),\n    Plot.dot(\n      data[`${N_}_${mu}_${nu_n}_${nu_p}_${alpha}_${beta}`], {\n        x: 'timesteps', y: \"value\", stroke: \"N\"\n        })\n  ]\n})"
  },
  {
    "objectID": "posts/sci-group-life-cycle/index.html#takeaways",
    "href": "posts/sci-group-life-cycle/index.html#takeaways",
    "title": "Scientific group life cycle",
    "section": "Takeaways:",
    "text": "Takeaways:"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]